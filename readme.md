# CellReasoner: A reasoning-enhanced large language model for cell type annotation ğŸ§¬ğŸ§ 

<div align="center">

[ğŸ“„ Paper](https://www.biorxiv.org/content/10.1101/2025.05.20.655112v1) | [ğŸ“Š Dataset](#-training-data) | [ğŸ¤– Model](https://huggingface.co/guangshuo/CellReasoner-7B)

</div>

---

<div align="center">
  <img src="figures/fig1.png" alt="CellReasoner Overview" width="80%">
</div>


---
## ğŸ“Œ Table of Contents

- [ğŸ“– CellReasoner: A reasoning-enhanced large language model for cell type annotation ğŸ§¬ğŸ§ ](#cellreasoner-a-reasoning-enhanced-large-language-model-for-cell-type-annotation-ğŸ§¬ğŸ§ )
  - [ğŸ“Œ Table of Contents](#-table-of-contents)
  - [ğŸ”¬ Key Highlights](#-key-highlights)
  - [ğŸ”‘ Key Results](#-key-results)
  - [ğŸ§  Model Zoo](#-model-zoo)
  - [ğŸ‹ï¸â€â™‚ï¸ Training](#-training)
  - [ğŸ“š Training Data](#-training-data)
  - [ğŸš€ Usage](#-usage)
  - [ğŸ“š Citation](#citation)

---

### ğŸ”¬ Key Highlights

- Only **a few expert-level reasoning samples** are needed to activate reasoning in a 7B LLM.
- **CellReasoner** achieves **expert-level interpretability** and **zero-/few-shot generalization**.
- Demonstrated **superior performance** across various **scRNA-seq** and **scATAC-seq** datasets.
- Compatible with **marker-by-marker annotation**, **ontology mapping**, and **biological reasoning**.

> ğŸ§  Less data, more reasoning: CellReasoner achieves accurate, interpretable, and scalable cell annotation with minimal supervision.

---

## ğŸ”‘ Key Results

### [PDAC dataset](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE197177)

| Model               | Score |
|--------------------|-------|
| Deepseek-V3        | 0.50  |
| Deepseek-R1        | 0.53  |
| ChatGPT-o3         | 0.58  |
| ChatGPT-4o         | 0.63  |
| singleR            | 0.68  |
| **CellReasoner-7B**  | **0.73** |
| **CellReasoner-32B** | **0.74** |

---

### [PBMC3K dataset](https://www.10xgenomics.com/cn/datasets/3-k-pbm-cs-from-a-healthy-donor-1-standard-1-1-0)

| Model               | Score |
|--------------------|-------|
| Deepseek-V3        | 0.52  |
| Deepseek-R1        | 0.52  |
| ChatGPT-4o         | 0.76  |
| ChatGPT-o3         | 0.85  |
| singleR            | 0.83  |
| **CellReasoner-7B**  | **0.87** |
| **CellReasoner-32B** | **0.84** |

---

## ğŸ§  Model Zoo

Our CellReasoner models are available on Hugging Face ğŸ¤—:

| Model                | Backbone                   | Link |
|---------------------|----------------------------|------|
| **CellReasoner-7B**  | [Qwen2.5-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct)         | [ğŸ¤—](https://huggingface.co/guangshuo/CellReasoner-7B) |
| **CellReasoner-32B** | [QwQ-32B](https://huggingface.co/Qwen/QwQ-32B)      | [ğŸ¤—](https://huggingface.co/guangshuo/CellReasoner-32B) |

---

## ğŸ‹ï¸â€â™‚ï¸ Training

We use the [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory) framework for fine-tuning. It offers a flexible and efficient pipeline for supervised fine-tuning, LoRA, and multi-stage training strategies.

---

## ğŸ“š Training Data

We adopt a **three-stage training strategy** combining reasoning scaffold, biological knowledge infusion, and reasoning mode fusion.

| Dataset Name   | Training Stage         | Samples |
|----------------|------------------------|----------|
| **CellCoT**       | Reasoning Scaffold, Reasoning Mode Fusion       | 380      |
| **pancancer38k**  | Knowledge Infusion        | 37,187        |
| **pancancer4k**   |   Internal test dataset   | 3,800    |

You can download the datasets from [here](https://biobigdata.nju.edu.cn/CellReasonerDataset/).


## ğŸš€ Usage

### ğŸ› ï¸ Step 1: Prepare Conda Environment

Make sure you have a working conda environment with the necessary dependencies installed. We recommend:

```bash
conda create -n cellreasoner python=3.11
conda activate cellreasoner
pip install -r requirements.txt
```

---

### ğŸ§ª Step 2: Preprocess Input Data

If your input is in **Seurat `.rds`** format, use the R preprocessing script:

```bash
Rscript s01.process_rds.R ./demo_data/pbmc_demo.rds ./output/ data/ranked_hvg.list
```

If your input is in **AnnData `.h5ad`** format, use the Python script:

```bash
python s01.process_h5ad.py \
    --input_file ./demo_data/pbmc_demo.h5ad \
    --output_path ./output_h5ad \
    --ranked_hvg_list ./data/ranked_hvg.list
```

Both pipelines will generate the following output files:

```
output/
â”œâ”€â”€ pbmc_demo.h5
â””â”€â”€ pbmc_demo.meta.csv
```

---

### ğŸ§± Step 3: Build Dataset for CellReasoner

Build the model input file using:

```bash
python s02.build_dataset.py \
    --h5_path ./output/pbmc_demo.h5 \
    --output_path ./output/ \
    --meta_file_path ./output/pbmc_demo.meta.csv
```

If your metadata includes cell type labels (for scoring), specify the column name:

```bash
python s02.build_dataset.py \
    --h5_path ./output/pbmc_demo.h5 \
    --output_path ./output/ \
    --meta_file_path ./output/pbmc_demo.meta.csv \
    --cell_type_column "seurat_annotations"
```

This will generate:

```
output/
â””â”€â”€ pbmc_demo_for_CellReasoner.json
```

---

### ğŸ¤– Step 4: Run Inference with CellReasoner

```bash
python s03.inference.py \
    --model "CellReasoner-7B" \
    --output_path "./output" \
    --input_json "./output/pbmc_demo_for_CellReasoner.json" \
    --batch_size 2
```

Result:

```
output/
â””â”€â”€ pbmc_demo_CellReasoner_result.csv
```

---

### ğŸ“Š Evaluation and Reasoning Visualization

To compute scores, generate plots, or view reasoning outputs, refer to:

```bash
s03.inference.ipynb
```



## Citation

```bibtex
@article {Cao2025.05.20.655112,
	author = {Cao, Guangshuo and Shen, Yi and Wu, Jianghong and Chao, Haoyu and Chen, Ming and Chen, Dijun},
	title = {CellReasoner: A reasoning-enhanced large language model for cell type annotation},
	elocation-id = {2025.05.20.655112},
	year = {2025},
	doi = {10.1101/2025.05.20.655112},
	URL = {https://www.biorxiv.org/content/early/2025/05/26/2025.05.20.655112},
	eprint = {https://www.biorxiv.org/content/early/2025/05/26/2025.05.20.655112.full.pdf},
	journal = {bioRxiv}
}
```

---


